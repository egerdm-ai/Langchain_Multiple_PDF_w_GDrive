{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLsVxMJNxo0x/2t7WlEr/g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/egerdm-ai/Langchain_Multiple_PDF_w_GDrive/blob/main/WFDF_Rules_QA_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai chromadb tiktoken pypdf"
      ],
      "metadata": {
        "id": "YX2rcDpJHGv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yzCSVm2WHCbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEIttU8vGpcE"
      },
      "outputs": [],
      "source": [
        "#this is for building the pdf qa from drive\n",
        "import os\n",
        "import sys\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import Docx2txtLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "root_dir = \"/content/drive/MyDrive/\"\n",
        "documents = []\n",
        "# Create a List of Documents from all of our files in the ./docs folder\n",
        "for file in os.listdir(root_dir+\"pdf\"):\n",
        "    if file.endswith(\".pdf\"):\n",
        "        pdf_path = root_dir+\"pdf/\" + file\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        documents.extend(loader.load())\n",
        "    elif file.endswith('.docx') or file.endswith('.doc'):\n",
        "        doc_path = root_dir+\"pdf/\" + file\n",
        "        loader = Docx2txtLoader(doc_path)\n",
        "        documents.extend(loader.load())\n",
        "    elif file.endswith('.txt'):\n",
        "        text_path = root_dir+\"pdf/\" + file\n",
        "        loader = TextLoader(text_path)\n",
        "        documents.extend(loader.load())\n",
        "\n",
        "# Split the documents into smaller chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 2000,\n",
        "    chunk_overlap = 100,\n",
        "    length_function = len,\n",
        "    add_start_index = True,\n",
        ")\n",
        "documents = text_splitter.split_documents(documents)\n",
        "\n",
        "# Convert the document chunks to embedding and save them to the vector store\n",
        "vectordb = Chroma.from_documents(documents, embedding=OpenAIEmbeddings(), persist_directory=root_dir+\"data\")\n",
        "vectordb.persist()\n",
        "\n",
        "# create our Q&A chain\n",
        "pdf_qa = ConversationalRetrievalChain.from_llm(\n",
        "    ChatOpenAI(temperature=0.7, model_name='gpt-3.5-turbo'),\n",
        "    retriever=vectordb.as_retriever(search_kwargs={'k': 6}),\n",
        "    return_source_documents=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this is for running the qa in colab\n",
        "\n",
        "yellow = \"\\033[0;33m\"\n",
        "green = \"\\033[0;32m\"\n",
        "white = \"\\033[0;39m\"\n",
        "\n",
        "chat_history = []\n",
        "print(f\"{yellow}------------------------------------------------\")\n",
        "print('You can start asking questions about your files')\n",
        "print('-----------------------------------------------')\n",
        "MAX_HISTORY = 5  # The maximum number of exchanges to include in the history.\n",
        "\n",
        "while True:\n",
        "    query = input(f\"{green}Prompt: \")\n",
        "    if query == \"exit\" or query == \"quit\" or query == \"q\" or query == \"f\":\n",
        "        print('Exiting')\n",
        "        sys.exit()\n",
        "    if query == '':\n",
        "        continue\n",
        "\n",
        "    # Only include the last `MAX_HISTORY` exchanges in the chat history.\n",
        "    recent_history = chat_history[-MAX_HISTORY:]\n",
        "\n",
        "    # Combine the recent exchanges into one string, with each question and answer on a new line.\n",
        "    chat_history_text = \"\\n\".join([f\"User: {q}\\nAI: {a}\" for q, a in recent_history])\n",
        "\n",
        "    # Include the current question at the end of the conversation history.\n",
        "    full_text = f\"{chat_history_text}\\nUser: {query}\"\n",
        "\n",
        "    # Pass `full_text` to your model instead of just the question.\n",
        "    result = pdf_qa({\"question\": full_text, \"chat_history\": chat_history})\n",
        "\n",
        "    print(f\"{white}Answer: \" + result[\"answer\"])\n",
        "    chat_history.append((query, result[\"answer\"]))"
      ],
      "metadata": {
        "id": "CUcnpbW5G6gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is for api\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Create a global variable to store chat history\n",
        "chat_history = []\n",
        "MAX_HISTORY = 5\n",
        "@app.route('/ask', methods=['POST'])\n",
        "def ask():\n",
        "    global chat_history\n",
        "\n",
        "    data = request.get_json()\n",
        "    query = data['query']\n",
        "\n",
        "    # Only include the last `MAX_HISTORY` exchanges in the chat history.\n",
        "    recent_history = chat_history[-MAX_HISTORY:]\n",
        "\n",
        "    # Combine the recent exchanges into one string, with each question and answer on a new line.\n",
        "    chat_history_text = \"\\n\".join([f\"User: {q}\\nAI: {a}\" for q, a in recent_history])\n",
        "\n",
        "    # Include the current question at the end of the conversation history.\n",
        "    full_text = f\"{chat_history_text}\\nUser: {query}\"\n",
        "\n",
        "    # Fetch the pdf_qa object from cache and ask the question.\n",
        "    result = pdf_qa({\"question\": full_text, \"chat_history\": \"\"})\n",
        "\n",
        "    # Append the current question and answer to the chat history.\n",
        "    chat_history.append((query, result['answer']))\n",
        "\n",
        "    response = {\n",
        "        'answer': result['answer']\n",
        "    }\n",
        "\n",
        "    return jsonify(response)\n",
        "\n",
        "\n",
        "# Setup ngrok and start the Flask application\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"Your API is accessible publicly via this URL:\", public_url)\n",
        "app.run(port=5000)\n"
      ],
      "metadata": {
        "id": "sY65hbCgHSNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is an example request\n",
        "\n",
        "import requests\n",
        "\n",
        "data = {\n",
        "    'query': \"why?\"\n",
        "}\n",
        "\n",
        "response = requests.post('https://b400-34-86-206-243.ngrok-free.app/ask', json=data)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print('Response from the server:')\n",
        "    print(response.json())\n",
        "else:\n",
        "    print(f'Request failed with status code {response.status_code}')"
      ],
      "metadata": {
        "id": "JTgNUdMvHVx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is for creating streamlit app using api\n",
        "\n",
        "import streamlit as st\n",
        "import requests\n",
        "\n",
        "\n",
        "# Function to send POST request to your Flask API\n",
        "def send_query(query):\n",
        "    # Add a template to guide the AI's answer\n",
        "    query_with_template = f\"{query}. Please provide an answer according to the content of the rules, specifying the rule and its index. \"\n",
        "    data = {\n",
        "        'query': query_with_template\n",
        "    }\n",
        "    url = st.secrets[\"url\"]\n",
        "    response = requests.post(url, json=data)\n",
        "    return response.json()['answer']\n",
        "\n",
        "# Chat history to display the conversation\n",
        "chat_history = []\n",
        "\n",
        "# Streamlit layout\n",
        "st.title(\"WFDF Rules AI\")\n",
        "\n",
        "user_input = st.text_input(\"You can ask questions about the rules from here: \",\"Is it considered a foul if the thrower's hand hits the marker after the throw?\")\n",
        "\n",
        "if st.button('Send'):\n",
        "    # Get the API response\n",
        "    response = send_query(user_input)\n",
        "\n",
        "    # Add the user query and API response to the chat history\n",
        "    chat_history.append(('You', user_input))\n",
        "    chat_history.append(('AI', response))\n",
        "\n",
        "    # Display the chat history\n",
        "    for chat in chat_history:\n",
        "        st.markdown(f\"**{chat[0]}**: {chat[1]}\")\n"
      ],
      "metadata": {
        "id": "svs1K3W9HmRF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}