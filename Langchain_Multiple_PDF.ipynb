{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZugpNRJdyyQx4IpGywd/r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/egerdm-ai/Langchain_Multiple_PDF_w_GDrive/blob/main/Langchain_Multiple_PDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw5MQJhSfKF-"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai chromadb tiktoken pypdf GoogleNews fake_useragent newspaper3k"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bL34-fHVXIvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install newspaper3k\n",
        "# Import necessary libraries\n",
        "from GoogleNews import GoogleNews\n",
        "import pandas as pd\n",
        "import requests\n",
        "from fake_useragent import UserAgent\n",
        "import newspaper\n",
        "from newspaper import fulltext\n",
        "import re\n",
        "# Define the keyword to search.\n",
        "keyword = 'gpt'\n",
        "\n",
        "# Perform news scraping from Google and extract the result into Pandas dataframe.\n",
        "googlenews = GoogleNews(lang='en', region='US', period='1d', encode='utf-8')\n",
        "googlenews.clear()\n",
        "googlenews.search(keyword)\n",
        "googlenews.get_page(2)\n",
        "news_result = googlenews.result(sort=True)\n",
        "news_data_df = pd.DataFrame.from_dict(news_result)\n",
        "\n",
        "ua = UserAgent()\n",
        "news_data_df_with_text = []\n",
        "for index, headers in news_data_df.iterrows():\n",
        "    news_title = str(headers['title'])\n",
        "    news_media = str(headers['media'])\n",
        "    news_update = str(headers['date'])\n",
        "    news_timestamp = str(headers['datetime'])\n",
        "    news_description = str(headers['desc'])\n",
        "    news_link = str(headers['link'])\n",
        "    print(news_link)\n",
        "    news_img = str(headers['img'])\n",
        "    try:\n",
        "        # html = requests.get(news_link).text\n",
        "        html = requests.get(news_link, headers={'User-Agent':ua.chrome}, timeout=5).text\n",
        "        text = fulltext(html)\n",
        "        print('Text Content Scraped')\n",
        "    except:\n",
        "        print('Text Content Scraped Error, Skipped')\n",
        "        pass\n",
        "    news_data_df_with_text.append([news_title, news_media, news_update, news_timestamp,\n",
        "                                         news_description, news_link, news_img, text])\n",
        "\n",
        "news_data_with_text_df = pd.DataFrame(news_data_df_with_text, columns=['Title', 'Media', 'Update', 'Timestamp',\n",
        "                                                                    'Description', 'Link', 'Image', 'Text'])\n",
        "\n",
        "# Display the entire dataframe for sample checking.\n",
        "news_data_with_text_df\n",
        "with open(\"/content/drive/MyDrive/data/recent_news.txt\", \"w\") as f:\n",
        "    f.write(f\"Document Title: {'Recent News About GPT'}\\n\")\n",
        "    for i, row in news_data_with_text_df.iterrows():\n",
        "        f.write(f\"Title: {row['Title']}\\n\")\n",
        "        f.write(f\"Media: {row['Media']}\\n\")\n",
        "        f.write(f\"Timestamp: {row['Timestamp']}\\n\")\n",
        "        f.write(f\"Description: {row['Description']}\\n\")\n",
        "        f.write(f\"Link: {row['Link']}\\n\")\n",
        "        f.write(f\"Text: {row['Text']}\\n\")\n",
        "        f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "n4A2ikqAVuR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import Docx2txtLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'YOUR-OPENAI-API-KEY'\n",
        "root_dir = \"/content/drive/MyDrive/\"\n",
        "documents = []\n",
        "# Create a List of Documents from all of our files in the ./docs folder\n",
        "for file in os.listdir(root_dir+\"data\"):\n",
        "    if file.endswith(\".pdf\"):\n",
        "        pdf_path = root_dir+\"data/\" + file\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        documents.extend(loader.load())\n",
        "    elif file.endswith('.docx') or file.endswith('.doc'):\n",
        "        doc_path = root_dir+\"data/\" + file\n",
        "        loader = Docx2txtLoader(doc_path)\n",
        "        documents.extend(loader.load())\n",
        "    elif file.endswith('.txt'):\n",
        "        text_path = root_dir+\"data/\" + file\n",
        "        loader = TextLoader(text_path)\n",
        "        documents.extend(loader.load())\n",
        "\n",
        "# Split the documents into smaller chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 2000,\n",
        "    chunk_overlap = 100,\n",
        "    length_function = len,\n",
        "    add_start_index = True,\n",
        ")\n",
        "documents = text_splitter.split_documents(documents)\n",
        "\n",
        "# Convert the document chunks to embedding and save them to the vector store\n",
        "vectordb = Chroma.from_documents(documents, embedding=OpenAIEmbeddings(), persist_directory=root_dir+\"data\")\n",
        "vectordb.persist()\n",
        "\n",
        "# create our Q&A chain\n",
        "pdf_qa = ConversationalRetrievalChain.from_llm(\n",
        "    ChatOpenAI(temperature=0.7, model_name='gpt-3.5-turbo'),\n",
        "    retriever=vectordb.as_retriever(search_kwargs={'k': 6}),\n",
        "    return_source_documents=True,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "yellow = \"\\033[0;33m\"\n",
        "green = \"\\033[0;32m\"\n",
        "white = \"\\033[0;39m\"\n",
        "\n",
        "chat_history = []\n",
        "print(f\"{yellow}------------------------------------------------\")\n",
        "print('You can start asking questions about your files')\n",
        "print('-----------------------------------------------')\n",
        "while True:\n",
        "    query = input(f\"{green}Prompt: \")\n",
        "    if query == \"exit\" or query == \"quit\" or query == \"q\" or query == \"f\":\n",
        "        print('Exiting')\n",
        "        sys.exit()\n",
        "    if query == '':\n",
        "        continue\n",
        "    result = pdf_qa(\n",
        "        {\"question\": query, \"chat_history\": chat_history})\n",
        "    print(f\"{white}Answer: \" + result[\"answer\"])\n",
        "    chat_history.append((query, result[\"answer\"]))"
      ],
      "metadata": {
        "id": "61qHTc4KfLM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R4FE_OswgkMj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}